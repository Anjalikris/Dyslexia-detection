{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python Version- 3.6.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(\"Current Python Version-\", python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 Version: 2.7.1\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "print(\"HDF5 Version:\", h5py.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 64)        3136      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 6, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 2, 64)         61504     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 2, 64)          16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 5, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 92,002\n",
      "Trainable params: 91,746\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Found 733 images belonging to 2 classes.\n",
      "Found 228 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 2s 521ms/step - loss: 9.0193 - acc: 0.5000 - val_loss: 7.9827 - val_acc: 0.8750\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 110ms/step - loss: 8.0023 - acc: 0.7812 - val_loss: 7.4222 - val_acc: 1.0000\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 7.5973 - acc: 0.9062 - val_loss: 7.2796 - val_acc: 1.0000\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 7.3682 - acc: 0.9297 - val_loss: 7.0369 - val_acc: 1.0000\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 7.1129 - acc: 0.9297 - val_loss: 6.8600 - val_acc: 1.0000\n",
      "Epoch 6/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 6.8897 - acc: 0.9523 - val_loss: 6.7249 - val_acc: 0.9375\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 6.6898 - acc: 0.9531 - val_loss: 6.4491 - val_acc: 1.0000\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 6.4590 - acc: 0.9453 - val_loss: 6.2287 - val_acc: 1.0000\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 6.1720 - acc: 0.9844 - val_loss: 5.9856 - val_acc: 1.0000\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 6.0194 - acc: 0.9609 - val_loss: 5.7868 - val_acc: 1.0000\n",
      "Epoch 11/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 5.7578 - acc: 0.9766 - val_loss: 5.6773 - val_acc: 0.9375\n",
      "Epoch 12/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 5.5484 - acc: 0.9680 - val_loss: 5.4245 - val_acc: 0.9375\n",
      "Epoch 13/150\n",
      "4/4 [==============================] - 0s 112ms/step - loss: 5.4234 - acc: 0.9531 - val_loss: 5.1483 - val_acc: 1.0000\n",
      "Epoch 14/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 5.0837 - acc: 1.0000 - val_loss: 4.9385 - val_acc: 1.0000\n",
      "Epoch 15/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 4.9044 - acc: 0.9922 - val_loss: 4.7468 - val_acc: 1.0000\n",
      "Epoch 16/150\n",
      "4/4 [==============================] - 0s 114ms/step - loss: 4.7735 - acc: 0.9688 - val_loss: 4.5539 - val_acc: 1.0000\n",
      "Epoch 17/150\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 4.5250 - acc: 0.9922 - val_loss: 4.3685 - val_acc: 1.0000\n",
      "Epoch 18/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 4.3423 - acc: 0.9758 - val_loss: 4.2471 - val_acc: 0.9375\n",
      "Epoch 19/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 4.1805 - acc: 0.9844 - val_loss: 4.0295 - val_acc: 1.0000\n",
      "Epoch 20/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 3.9907 - acc: 0.9922 - val_loss: 3.8934 - val_acc: 1.0000\n",
      "Epoch 21/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 3.8244 - acc: 0.9922 - val_loss: 3.6950 - val_acc: 1.0000\n",
      "Epoch 22/150\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 3.6463 - acc: 1.0000 - val_loss: 3.5393 - val_acc: 1.0000\n",
      "Epoch 23/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 3.4938 - acc: 0.9921 - val_loss: 3.3923 - val_acc: 1.0000\n",
      "Epoch 24/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 3.3458 - acc: 1.0000 - val_loss: 3.2392 - val_acc: 1.0000\n",
      "Epoch 25/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 3.2037 - acc: 0.9922 - val_loss: 3.2253 - val_acc: 0.9375\n",
      "Epoch 26/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 3.0639 - acc: 0.9922 - val_loss: 2.9724 - val_acc: 1.0000\n",
      "Epoch 27/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 2.9113 - acc: 1.0000 - val_loss: 2.8328 - val_acc: 1.0000\n",
      "Epoch 28/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 2.7858 - acc: 1.0000 - val_loss: 2.7207 - val_acc: 1.0000\n",
      "Epoch 29/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 2.6646 - acc: 1.0000 - val_loss: 2.6150 - val_acc: 1.0000\n",
      "Epoch 30/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 2.5380 - acc: 1.0000 - val_loss: 2.4791 - val_acc: 1.0000\n",
      "Epoch 31/150\n",
      "4/4 [==============================] - 0s 112ms/step - loss: 2.4448 - acc: 0.9922 - val_loss: 2.3768 - val_acc: 1.0000\n",
      "Epoch 32/150\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 2.3210 - acc: 1.0000 - val_loss: 2.2633 - val_acc: 1.0000\n",
      "Epoch 33/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 2.2464 - acc: 0.9844 - val_loss: 2.2497 - val_acc: 1.0000\n",
      "Epoch 34/150\n",
      "4/4 [==============================] - 0s 111ms/step - loss: 2.1264 - acc: 0.9844 - val_loss: 2.0833 - val_acc: 1.0000\n",
      "Epoch 35/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 2.0229 - acc: 1.0000 - val_loss: 1.9686 - val_acc: 1.0000\n",
      "Epoch 36/150\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 1.9285 - acc: 1.0000 - val_loss: 1.8791 - val_acc: 1.0000\n",
      "Epoch 37/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 1.8539 - acc: 0.9922 - val_loss: 1.8116 - val_acc: 1.0000\n",
      "Epoch 38/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 105ms/step - loss: 1.7597 - acc: 1.0000 - val_loss: 1.7085 - val_acc: 1.0000\n",
      "Epoch 39/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 1.7059 - acc: 0.9922 - val_loss: 1.6587 - val_acc: 1.0000\n",
      "Epoch 40/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 1.6033 - acc: 1.0000 - val_loss: 1.6216 - val_acc: 1.0000\n",
      "Epoch 41/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 1.5351 - acc: 1.0000 - val_loss: 1.5130 - val_acc: 1.0000\n",
      "Epoch 42/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 1.4650 - acc: 1.0000 - val_loss: 1.4558 - val_acc: 1.0000\n",
      "Epoch 43/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 1.3966 - acc: 1.0000 - val_loss: 1.3601 - val_acc: 1.0000\n",
      "Epoch 44/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 1.3314 - acc: 1.0000 - val_loss: 1.3265 - val_acc: 1.0000\n",
      "Epoch 45/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 1.2693 - acc: 1.0000 - val_loss: 1.2810 - val_acc: 1.0000\n",
      "Epoch 46/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 1.2093 - acc: 1.0000 - val_loss: 1.2070 - val_acc: 1.0000\n",
      "Epoch 47/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 1.1568 - acc: 1.0000 - val_loss: 1.1661 - val_acc: 1.0000\n",
      "Epoch 48/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 1.1013 - acc: 1.0000 - val_loss: 1.1139 - val_acc: 1.0000\n",
      "Epoch 49/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 1.0530 - acc: 1.0000 - val_loss: 1.0951 - val_acc: 1.0000\n",
      "Epoch 50/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 1.0062 - acc: 1.0000 - val_loss: 1.0455 - val_acc: 1.0000\n",
      "Epoch 51/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.9798 - acc: 0.9922 - val_loss: 1.0938 - val_acc: 1.0000\n",
      "Epoch 52/150\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 0.9285 - acc: 1.0000 - val_loss: 1.0079 - val_acc: 1.0000\n",
      "Epoch 53/150\n",
      "4/4 [==============================] - 0s 110ms/step - loss: 0.9326 - acc: 0.9688 - val_loss: 1.0557 - val_acc: 0.9375\n",
      "Epoch 54/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.8912 - acc: 0.9766 - val_loss: 0.8319 - val_acc: 1.0000\n",
      "Epoch 55/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.8506 - acc: 0.9844 - val_loss: 0.8848 - val_acc: 1.0000\n",
      "Epoch 56/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.8121 - acc: 0.9922 - val_loss: 0.7939 - val_acc: 1.0000\n",
      "Epoch 57/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.7906 - acc: 0.9844 - val_loss: 0.8559 - val_acc: 0.8750\n",
      "Epoch 58/150\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 0.7452 - acc: 0.9921 - val_loss: 0.7176 - val_acc: 1.0000\n",
      "Epoch 59/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.7172 - acc: 0.9922 - val_loss: 0.6779 - val_acc: 1.0000\n",
      "Epoch 60/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.6881 - acc: 1.0000 - val_loss: 0.7014 - val_acc: 1.0000\n",
      "Epoch 61/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.6711 - acc: 0.9922 - val_loss: 0.6769 - val_acc: 1.0000\n",
      "Epoch 62/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.6246 - acc: 1.0000 - val_loss: 0.8117 - val_acc: 0.8750\n",
      "Epoch 63/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.6042 - acc: 1.0000 - val_loss: 0.5990 - val_acc: 1.0000\n",
      "Epoch 64/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.5801 - acc: 1.0000 - val_loss: 0.5560 - val_acc: 1.0000\n",
      "Epoch 65/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.5451 - acc: 1.0000 - val_loss: 0.5353 - val_acc: 1.0000\n",
      "Epoch 66/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.5238 - acc: 1.0000 - val_loss: 0.5069 - val_acc: 1.0000\n",
      "Epoch 67/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.5109 - acc: 0.9922 - val_loss: 0.4885 - val_acc: 1.0000\n",
      "Epoch 68/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.4820 - acc: 1.0000 - val_loss: 0.4761 - val_acc: 1.0000\n",
      "Epoch 69/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.4726 - acc: 0.9921 - val_loss: 0.4554 - val_acc: 1.0000\n",
      "Epoch 70/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.4379 - acc: 1.0000 - val_loss: 0.4637 - val_acc: 1.0000\n",
      "Epoch 71/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.4286 - acc: 1.0000 - val_loss: 0.4773 - val_acc: 1.0000\n",
      "Epoch 72/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 0.4160 - acc: 0.9922 - val_loss: 0.4346 - val_acc: 1.0000\n",
      "Epoch 73/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.3905 - acc: 1.0000 - val_loss: 0.4189 - val_acc: 1.0000\n",
      "Epoch 74/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.3722 - acc: 1.0000 - val_loss: 0.3918 - val_acc: 1.0000\n",
      "Epoch 75/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 0.3555 - acc: 1.0000 - val_loss: 0.3861 - val_acc: 1.0000\n",
      "Epoch 76/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.3420 - acc: 1.0000 - val_loss: 0.4094 - val_acc: 1.0000\n",
      "Epoch 77/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.3268 - acc: 1.0000 - val_loss: 0.3642 - val_acc: 1.0000\n",
      "Epoch 78/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.3131 - acc: 1.0000 - val_loss: 0.3780 - val_acc: 1.0000\n",
      "Epoch 79/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.3035 - acc: 1.0000 - val_loss: 0.3416 - val_acc: 1.0000\n",
      "Epoch 80/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.2910 - acc: 1.0000 - val_loss: 0.3481 - val_acc: 1.0000\n",
      "Epoch 81/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.2805 - acc: 1.0000 - val_loss: 0.3296 - val_acc: 1.0000\n",
      "Epoch 82/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.2721 - acc: 1.0000 - val_loss: 0.3667 - val_acc: 1.0000\n",
      "Epoch 83/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.2589 - acc: 1.0000 - val_loss: 0.4041 - val_acc: 1.0000\n",
      "Epoch 84/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.2595 - acc: 1.0000 - val_loss: 0.3437 - val_acc: 1.0000\n",
      "Epoch 85/150\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 0.2397 - acc: 1.0000 - val_loss: 0.2884 - val_acc: 1.0000\n",
      "Epoch 86/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.2316 - acc: 1.0000 - val_loss: 0.3063 - val_acc: 1.0000\n",
      "Epoch 87/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.2268 - acc: 1.0000 - val_loss: 0.2723 - val_acc: 1.0000\n",
      "Epoch 88/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.2155 - acc: 1.0000 - val_loss: 0.3517 - val_acc: 1.0000\n",
      "Epoch 89/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.2131 - acc: 1.0000 - val_loss: 0.3180 - val_acc: 1.0000\n",
      "Epoch 90/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.2067 - acc: 1.0000 - val_loss: 0.2765 - val_acc: 1.0000\n",
      "Epoch 91/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 0.2020 - acc: 1.0000 - val_loss: 0.3921 - val_acc: 0.9375\n",
      "Epoch 92/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.2362 - acc: 0.9765 - val_loss: 0.2514 - val_acc: 1.0000\n",
      "Epoch 93/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.2558 - acc: 0.9688 - val_loss: 0.1986 - val_acc: 1.0000\n",
      "Epoch 94/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.2260 - acc: 0.9922 - val_loss: 0.2060 - val_acc: 1.0000\n",
      "Epoch 95/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.2144 - acc: 1.0000 - val_loss: 0.2186 - val_acc: 1.0000\n",
      "Epoch 96/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.2211 - acc: 1.0000 - val_loss: 0.1984 - val_acc: 1.0000\n",
      "Epoch 97/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.2253 - acc: 0.9922 - val_loss: 0.1966 - val_acc: 1.0000\n",
      "Epoch 98/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.2070 - acc: 1.0000 - val_loss: 0.1930 - val_acc: 1.0000\n",
      "Epoch 99/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.1976 - acc: 1.0000 - val_loss: 0.1873 - val_acc: 1.0000\n",
      "Epoch 100/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 105ms/step - loss: 0.1974 - acc: 1.0000 - val_loss: 0.1814 - val_acc: 1.0000\n",
      "Epoch 101/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.1873 - acc: 1.0000 - val_loss: 0.1847 - val_acc: 1.0000\n",
      "Epoch 102/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.1776 - acc: 1.0000 - val_loss: 0.1683 - val_acc: 1.0000\n",
      "Epoch 103/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.1765 - acc: 1.0000 - val_loss: 0.1653 - val_acc: 1.0000\n",
      "Epoch 104/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.1632 - acc: 1.0000 - val_loss: 0.1733 - val_acc: 1.0000\n",
      "Epoch 105/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.1597 - acc: 1.0000 - val_loss: 0.1599 - val_acc: 1.0000\n",
      "Epoch 106/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.1507 - acc: 1.0000 - val_loss: 0.1414 - val_acc: 1.0000\n",
      "Epoch 107/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.1463 - acc: 1.0000 - val_loss: 0.1644 - val_acc: 1.0000\n",
      "Epoch 108/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.1390 - acc: 1.0000 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 109/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.1367 - acc: 1.0000 - val_loss: 0.1401 - val_acc: 1.0000\n",
      "Epoch 110/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.1304 - acc: 1.0000 - val_loss: 0.1435 - val_acc: 1.0000\n",
      "Epoch 111/150\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 0.1226 - acc: 1.0000 - val_loss: 0.1489 - val_acc: 1.0000\n",
      "Epoch 112/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.1212 - acc: 1.0000 - val_loss: 0.1385 - val_acc: 1.0000\n",
      "Epoch 113/150\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 0.1175 - acc: 1.0000 - val_loss: 0.1466 - val_acc: 1.0000\n",
      "Epoch 114/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.1145 - acc: 1.0000 - val_loss: 0.1266 - val_acc: 1.0000\n",
      "Epoch 115/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.1120 - acc: 1.0000 - val_loss: 0.1423 - val_acc: 1.0000\n",
      "Epoch 116/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.1059 - acc: 1.0000 - val_loss: 0.1492 - val_acc: 1.0000\n",
      "Epoch 117/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.1022 - acc: 1.0000 - val_loss: 0.1355 - val_acc: 1.0000\n",
      "Epoch 118/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.1057 - acc: 0.9922 - val_loss: 0.1518 - val_acc: 1.0000\n",
      "Epoch 119/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.0974 - acc: 1.0000 - val_loss: 0.1541 - val_acc: 1.0000\n",
      "Epoch 120/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.1002 - acc: 1.0000 - val_loss: 0.1926 - val_acc: 1.0000\n",
      "Epoch 121/150\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 0.0970 - acc: 1.0000 - val_loss: 0.1561 - val_acc: 1.0000\n",
      "Epoch 122/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 0.0921 - acc: 1.0000 - val_loss: 0.1239 - val_acc: 1.0000\n",
      "Epoch 123/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.0920 - acc: 1.0000 - val_loss: 0.1200 - val_acc: 1.0000\n",
      "Epoch 124/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 0.0902 - acc: 1.0000 - val_loss: 0.1844 - val_acc: 1.0000\n",
      "Epoch 125/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.0874 - acc: 1.0000 - val_loss: 0.1400 - val_acc: 1.0000\n",
      "Epoch 126/150\n",
      "4/4 [==============================] - 0s 114ms/step - loss: 0.0859 - acc: 1.0000 - val_loss: 0.1576 - val_acc: 1.0000\n",
      "Epoch 127/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.0831 - acc: 1.0000 - val_loss: 0.1419 - val_acc: 1.0000\n",
      "Epoch 128/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.0822 - acc: 1.0000 - val_loss: 0.1405 - val_acc: 1.0000\n",
      "Epoch 129/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.0776 - acc: 1.0000 - val_loss: 0.1749 - val_acc: 1.0000\n",
      "Epoch 130/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.0776 - acc: 1.0000 - val_loss: 0.1892 - val_acc: 1.0000\n",
      "Epoch 131/150\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.0770 - acc: 1.0000 - val_loss: 0.1323 - val_acc: 1.0000\n",
      "Epoch 132/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.0725 - acc: 1.0000 - val_loss: 0.1292 - val_acc: 1.0000\n",
      "Epoch 133/150\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 0.0720 - acc: 1.0000 - val_loss: 0.1317 - val_acc: 1.0000\n",
      "Epoch 134/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.0739 - acc: 1.0000 - val_loss: 0.1925 - val_acc: 1.0000\n",
      "Epoch 135/150\n",
      "4/4 [==============================] - 0s 111ms/step - loss: 0.0695 - acc: 1.0000 - val_loss: 0.1767 - val_acc: 1.0000\n",
      "Epoch 136/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.0689 - acc: 1.0000 - val_loss: 0.1129 - val_acc: 1.0000\n",
      "Epoch 137/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.0665 - acc: 1.0000 - val_loss: 0.1163 - val_acc: 1.0000\n",
      "Epoch 138/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 0.0651 - acc: 1.0000 - val_loss: 0.1487 - val_acc: 1.0000\n",
      "Epoch 139/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 0.0665 - acc: 1.0000 - val_loss: 0.1314 - val_acc: 1.0000\n",
      "Epoch 140/150\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 0.0642 - acc: 1.0000 - val_loss: 0.1224 - val_acc: 1.0000\n",
      "Epoch 141/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.0661 - acc: 1.0000 - val_loss: 0.1345 - val_acc: 1.0000\n",
      "Epoch 142/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.0685 - acc: 1.0000 - val_loss: 0.1482 - val_acc: 1.0000\n",
      "Epoch 143/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.0649 - acc: 1.0000 - val_loss: 0.1612 - val_acc: 1.0000\n",
      "Epoch 144/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.0631 - acc: 1.0000 - val_loss: 0.0952 - val_acc: 1.0000\n",
      "Epoch 145/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.0655 - acc: 1.0000 - val_loss: 0.1330 - val_acc: 1.0000\n",
      "Epoch 146/150\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 0.0627 - acc: 1.0000 - val_loss: 0.1005 - val_acc: 1.0000\n",
      "Epoch 147/150\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.0669 - acc: 1.0000 - val_loss: 0.0699 - val_acc: 1.0000\n",
      "Epoch 148/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.0618 - acc: 1.0000 - val_loss: 0.0654 - val_acc: 1.0000\n",
      "Epoch 149/150\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 0.0615 - acc: 1.0000 - val_loss: 0.0731 - val_acc: 1.0000\n",
      "Epoch 150/150\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 0.0630 - acc: 1.0000 - val_loss: 0.0757 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "NUM_EPOCHS = 150\n",
    "STEPS_PER_EPOCH_TRAINING = 4\n",
    "STEPS_PER_EPOCH_VALIDATION = 2\n",
    "BATCH_SIZE_TRAINING = 32\n",
    "BATCH_SIZE_VALIDATION = 8\n",
    "BATCH_SIZE_TESTING = 1\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def cnn(input_shape=(29,29,3)):\n",
    "  \n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(64, kernel_size=(4, 4), activation='relu', input_shape=input_shape))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "\n",
    "\t\n",
    "    \n",
    "\tmodel.add(Conv2D(64, (3, 5), activation='relu', kernel_regularizer=regularizers.l2(0.04)))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "\tmodel.add(Dropout(0.2))\n",
    "    \n",
    "\tmodel.add(Conv2D(64, (2, 2), activation='relu', padding='same'))\n",
    "\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "    \n",
    "\tmodel.add(Flatten())\n",
    "    \n",
    "\tmodel.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.04)))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.04)))\n",
    "    \n",
    "\tmodel.add(Dense(2, activation='softmax'))\n",
    "  \n",
    "\tmodel.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                  metrics=['accuracy'])\n",
    " \n",
    "\tmodel.summary()\n",
    "\treturn model\n",
    "\n",
    "model = cnn()\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "        'dysdata/train',\n",
    "        target_size=(29, 29),\n",
    "        batch_size=BATCH_SIZE_TRAINING,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "        'dysdata/test',\n",
    "        target_size=(29, 29),\n",
    "        batch_size=BATCH_SIZE_VALIDATION,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "fit_history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH_TRAINING,\n",
    "        epochs = NUM_EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=STEPS_PER_EPOCH_VALIDATION\n",
    ")\n",
    "\n",
    "model.save('model_cnn_dyslexia_mra.h5')\n",
    "\n",
    "#print(fit_history.history.keys())\n",
    "\n",
    "plt.figure(1, figsize = (15,8)) \n",
    "    \n",
    "plt.subplot(221)  \n",
    "plt.plot(fit_history.history['acc'])  \n",
    "plt.plot(fit_history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'valid']) \n",
    "    \n",
    "plt.subplot(222)  \n",
    "plt.plot(fit_history.history['loss'])  \n",
    "plt.plot(fit_history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'valid']) \n",
    "plt.savefig(\"accuracy.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "[[0.981729 0.018271]]\n",
      "[0]\n",
      "DYSLEXIC\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model_cnn_dyslexia_mra.h5')\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = data_generator.flow_from_directory(\n",
    "    directory='test_',\n",
    "    target_size=(29, 29),\n",
    "    batch_size=1,\n",
    "    class_mode=None,  # This should be None since you're not using labels\n",
    "    shuffle=False,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "pred = model.predict_generator(test_generator, steps=len(test_generator), verbose=1)\n",
    "print(pred)\n",
    "predicted_class_indices = np.argmax(pred, axis=1)\n",
    "print(predicted_class_indices)\n",
    "label = ['DYSLEXIC', 'NORMAL']\n",
    "print(label[predicted_class_indices[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
